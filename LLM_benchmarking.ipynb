{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2ZneMNUVZs-"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sz0DGA4oVToc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NebWSDiYVr8W"
      },
      "source": [
        "## Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSGPsINTVtnU"
      },
      "outputs": [],
      "source": [
        "df = pd.read_parquet(\"res.pqt\")\n",
        "df = df[df['subject'] == 'professional_law']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8oAT0vQK3Jc"
      },
      "source": [
        "## Topic Name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xjt_cOJCK5Sz"
      },
      "outputs": [],
      "source": [
        "topic_name = \"professional_law\"  # or \"professional_law\", \"high_school_macroeconomics\", or \"professional_psychology\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af4a15z-IUOq"
      },
      "source": [
        "## Function to calculate the mean of the \"corrects\" for different sample sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVkZg063Ia3v"
      },
      "outputs": [],
      "source": [
        "def compute_avg_correct_vs_sample_size(df, max_k=500, n=1000, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    means = []\n",
        "    for k in tqdm(range(1, max_k + 1), desc=\"Sample size loop\"):\n",
        "        trials = []\n",
        "        for _ in range(n):\n",
        "            sample = df.sample(n=k, replace=False)\n",
        "            trials.append(sample['correct'].mean())\n",
        "        means.append(np.mean(trials))\n",
        "    return means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP111DteIei5"
      },
      "source": [
        "## Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Gmb2ec7IfsB",
        "outputId": "a75619f1-f8fa-4096-ccfd-8c8e155a3c23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Sample size loop: 100%|██████████| 300/300 [02:01<00:00,  2.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall mean of 'correct': 0.6236138290932811\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "avg_means = compute_avg_correct_vs_sample_size(df, max_k=300, n=1000)\n",
        "overall_mean = df['correct'].mean()\n",
        "print(\"Overall mean of 'correct':\", overall_mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nPzytctNPjL"
      },
      "source": [
        "## Compute Sampling Baseline (Random)\n",
        "\n",
        "We compute the average \"correct\" score over multiple random subsets of increasing size (k = 1 to N), repeated multiple times to simulate stability. This provides a reference curve against which smarter selection strategies will be compared."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ImYrn4RNQso",
        "outputId": "43bbc85a-5fda-45b8-a834-df78a589bd9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [02:01<00:00,  2.47it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def compute_avg_correct_vs_sample_size(df, max_k=300, n=1000, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    means = []\n",
        "    for k in tqdm(range(1, max_k + 1)):\n",
        "        trials = []\n",
        "        for _ in range(n):\n",
        "            sample = df.sample(n=k, replace=False)\n",
        "            trials.append(sample['correct'].mean())\n",
        "        means.append(np.mean(trials))\n",
        "    return list(range(1, max_k + 1)), means\n",
        "\n",
        "# Filter dataset based on the selected topic\n",
        "df_topic = df[df[\"subject\"] == topic_name]\n",
        "\n",
        "# Run the sampling procedure\n",
        "k_values, avg_means = compute_avg_correct_vs_sample_size(df_topic, max_k=300, n=1000)\n",
        "overall_mean = df_topic[\"correct\"].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhScoTHhONBv"
      },
      "source": [
        "## Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Hj8ijbLN29h"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    \"k\": k_values,\n",
        "    \"avg_sample_mean\": avg_means\n",
        "})\n",
        "\n",
        "results_df.to_csv(f\"sample_mean_results_{topic_name}.csv\", index=False)\n",
        "results_df.to_excel(f\"sample_mean_results_{topic_name}.xlsx\", index=False)\n",
        "\n",
        "with open(f\"sample_mean_results_{topic_name}.json\", \"w\") as f:\n",
        "    json.dump({\n",
        "        \"k_values\": k_values,\n",
        "        \"avg_sample_mean\": avg_means,\n",
        "        \"overall_mean\": overall_mean,\n",
        "        \"topic\": topic_name\n",
        "    }, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9texzEKUyVZ"
      },
      "source": [
        "## KMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLMhN4SF7buG",
        "outputId": "55840eae-cfc5-444d-8a10-cf994b03c29e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "KMeans avg over 100 runs:  72%|███████▏  | 216/300 [3:45:30<2:42:39, 116.18s/it]"
          ]
        }
      ],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "\n",
        "def kmeans_subset(X, df, k, seed):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=seed, n_init='auto').fit(X)\n",
        "    centroids = kmeans.cluster_centers_\n",
        "    selected_idxs = []\n",
        "    for c in centroids:\n",
        "        distances = np.linalg.norm(X - c, axis=1)\n",
        "        best_idx = np.argmin(distances)\n",
        "        while best_idx in selected_idxs:\n",
        "            distances[best_idx] = np.inf\n",
        "            best_idx = np.argmin(distances)\n",
        "        selected_idxs.append(best_idx)\n",
        "    return df.iloc[selected_idxs]\n",
        "\n",
        "\n",
        "# Parameters and data loading\n",
        "# Automatically pick a topic with valid embeddings\n",
        "valid_topics = df[df[\"embedding\"].apply(lambda x: isinstance(x, (list, np.ndarray)) and len(x) > 0)][\"subject\"].unique()\n",
        "topic_name = valid_topics[0] if len(valid_topics) > 0 else None\n",
        "if topic_name is None:\n",
        "    raise ValueError(\"No topics found with valid embeddings\")\n",
        "df_topic = df[df[\"subject\"] == topic_name].copy()\n",
        "df_topic = df_topic[df_topic[\"embedding\"].apply(lambda x: isinstance(x, (list, np.ndarray)) and len(x) > 0)]\n",
        "if len(df_topic) == 0:\n",
        "    raise ValueError(f\"No valid embeddings found for topic '{topic_name}'\")\n",
        "X = np.vstack(df_topic[\"embedding\"].values)\n",
        "\n",
        "k_values = list(range(1, 301))\n",
        "n_repeats = 100\n",
        "kmeans_means = []\n",
        "\n",
        "# Main evaluation loop\n",
        "for k in tqdm(k_values, desc=f\"KMeans avg over {n_repeats} runs\"):\n",
        "    accuracies = []\n",
        "    for i in range(n_repeats):\n",
        "        selected_df = kmeans_subset(X, df_topic, k=k, seed=42 + i)\n",
        "        accuracies.append(selected_df[\"correct\"].mean())\n",
        "    kmeans_means.append(np.mean(accuracies))\n",
        "\n",
        "# Save results to CSV\n",
        "results_df = pd.DataFrame({\n",
        "    \"k\": k_values,\n",
        "    \"kmeans_avg_accuracy\": kmeans_means\n",
        "})\n",
        "results_df.to_csv(f\"kmeans_avg_results_{topic_name}.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bekTJJtyUzv9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set topic\n",
        "topic_name = \"professional_law\"  # e.g., \"professional_law\", \"high_school_macroeconomics\", \"professional_psychology\"\n",
        "\n",
        "# Load data\n",
        "df_random = pd.read_csv(f\"sample_mean_results_{topic_name}.csv\")\n",
        "df_kmeans = pd.read_csv(f\"kmeans_avg_results_{topic_name}.csv\")  # averaged KMeans results\n",
        "overall_mean = df[df[\"subject\"] == topic_name][\"correct\"].mean()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(df_random[\"k\"], df_random[\"avg_sample_mean\"],\n",
        "         label=\"Random Sampling (avg over 1000 runs)\", color=\"blue\")\n",
        "plt.plot(df_kmeans[\"k\"], df_kmeans[\"kmeans_avg_accuracy\"],\n",
        "         label=\"KMeans Clustering (avg over 10 runs)\", color=\"orange\")\n",
        "plt.axhline(y=overall_mean, color=\"gray\", linestyle=\"--\",\n",
        "            label=f\"Overall mean = {overall_mean:.3f}\")\n",
        "\n",
        "plt.xlabel(\"Subset size (k)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(f\"Accuracy vs Subset Size ({topic_name})\\nRandom vs KMeans\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.ylim(0.3, 1.0)  # standard Y-axis range for fair comparison\n",
        "plt.savefig(f\"comparison_plot_{topic_name}.png\", dpi=300)\n",
        "plt.show()\n",
        "\n",
        "# Note:\n",
        "# - KMeans results are averaged over 100 repetitions with different seeds.\n",
        "# - Random sampling is averaged over 1000 repetitions."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}